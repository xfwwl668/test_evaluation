"""DuckDB é«˜æ€§èƒ½è‚¡ç¥¨æ•°æ®åº“æ¨¡å—"""
import os
import time
from typing import Dict, List, Optional, Union
from datetime import date, datetime
from contextlib import contextmanager

import duckdb
import pandas as pd

# ==================== æ ¸å¿ƒæ•°æ®åº“ç±» ====================
class StockDatabase:
    """
    DuckDB è‚¡ç¥¨æ—¥çº¿æ•°æ®åº“
    
    æ¶æ„ç‰¹æ€§:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                  stocks_daily.db                   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚              daily_bars è¡¨                    â”‚  â”‚
    â”‚  â”‚  PK: (code, date) â†’ è‡ªåŠ¨å»é‡/UPSERT          â”‚  â”‚
    â”‚  â”‚  IDX: idx_date    â†’ å…¨å¸‚åœºå¿«ç…§ O(1)          â”‚  â”‚
    â”‚  â”‚  IDX: idx_code    â†’ å•è‚¡å†å² O(log n)        â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    TABLE = "daily_bars"
    
    DDL = f"""
        CREATE TABLE IF NOT EXISTS {TABLE} (
            code    VARCHAR(10) NOT NULL,
            market  TINYINT NOT NULL,
            date    DATE NOT NULL,
            name    VARCHAR(20),
            open    FLOAT,
            high    FLOAT,
            low     FLOAT,
            close   FLOAT,
            vol     BIGINT,
            amount  BIGINT,
            PRIMARY KEY (code, date)
        );
        CREATE INDEX IF NOT EXISTS idx_date ON {TABLE}(date);
        CREATE INDEX IF NOT EXISTS idx_code ON {TABLE}(code);
    """

    def __init__(self, db_path: str = "stocks_daily.db") -> None:
        self.db_path = db_path
        self._conn: Optional[duckdb.DuckDBPyConnection] = None
        self._init_schema()

    def _init_schema(self) -> None:
        """åˆå§‹åŒ–è¡¨ç»“æ„å’Œç´¢å¼•"""
        with self.connect() as conn:
            for stmt in self.DDL.split(';'):
                if stmt.strip():
                    conn.execute(stmt)

    @contextmanager
    def connect(self):
        """çº¿ç¨‹å®‰å…¨è¿æ¥ä¸Šä¸‹æ–‡"""
        conn = duckdb.connect(self.db_path)
        try:
            yield conn
        finally:
            conn.close()

    # ==================== é«˜ååå†™å…¥ ====================
    def bulk_upsert(self, df: pd.DataFrame) -> int:
        """
        æ‰¹é‡ UPSERT (INSERT OR REPLACE)
        """
        if df.empty:
            return 0
        
        df = self._normalize_df(df)
        
        with self.connect() as conn:
            conn.register("_tmp_df", df)
            conn.execute(f"""
                INSERT OR REPLACE INTO {self.TABLE}
                SELECT code, market, date, name, open, high, low, close, vol, amount
                FROM _tmp_df
            """)
            conn.unregister("_tmp_df")
        
        return len(df)
    
    upsert = bulk_upsert  # ğŸ”´ åˆ«åï¼Œä¸ºäº†å…¼å®¹ updater.py

    def bulk_upsert_appender(self, df: pd.DataFrame) -> int:
        """
        ä½¿ç”¨ Appender API æé€Ÿå†™å…¥ (é€‚åˆçº¯æ–°å¢åœºæ™¯)
        
        æ€§èƒ½: ~500K rows/sec
        """
        if df.empty:
            return 0
        
        df = self._normalize_df(df)
        
        with self.connect() as conn:
            # å…ˆåˆ é™¤å·²å­˜åœ¨çš„è®°å½• (å®ç° UPSERT)
            codes = df['code'].unique().tolist()
            dates = df['date'].unique().tolist()
            
            conn.execute(f"""
                DELETE FROM {self.TABLE}
                WHERE code IN ({','.join(['?']*len(codes))})
                  AND date IN ({','.join(['?']*len(dates))})
            """, codes + dates)
            
            # Appender æ‰¹é‡å†™å…¥
            appender = conn.appender(self.TABLE)
            for row in df.itertuples(index=False):
                appender.append_row(row)
            appender.close()
        
        return len(df)

    # ==================== å¢é‡æ›´æ–° ====================
    def incremental_update(self, data: Dict[str, pd.DataFrame]) -> Dict[str, int]:
        """
        æ‰¹é‡å¢é‡æ›´æ–°: åªè¿½åŠ æ¯”åº“ä¸­æ›´æ–°çš„æ•°æ®
        
        Args:
            data: {code: DataFrame} ä¸‹è½½ç»“æœå­—å…¸
        
        Returns:
            {code: æ–°å¢è¡Œæ•°}
        
        é€»è¾‘:
            1. æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰è‚¡ç¥¨æœ€æ–°æ—¥æœŸ (å•æ¬¡ SQL)
            2. æŒ‰æœ€æ–°æ—¥æœŸè¿‡æ»¤æ–°æ•°æ®
            3. åˆå¹¶åä¸€æ¬¡æ€§å†™å…¥
        """
        if not data:
            return {}
        
        codes = list(data.keys())
        results = {c: 0 for c in codes}
        
        with self.connect() as conn:
            # Step 1: æ‰¹é‡è·å–æœ€æ–°æ—¥æœŸ
            placeholders = ','.join(['?'] * len(codes))
            latest_df = conn.execute(f"""
                SELECT code, MAX(date) as latest
                FROM {self.TABLE}
                WHERE code IN ({placeholders})
                GROUP BY code
            """, codes).fetchdf()
            
            latest_map: Dict[str, date] = dict(
                zip(latest_df['code'], pd.to_datetime(latest_df['latest']).dt.date)
            ) if not latest_df.empty else {}
        
        # Step 2: è¿‡æ»¤æ–°æ•°æ®
        new_chunks: List[pd.DataFrame] = []
        
        for code, df in data.items():
            if df.empty:
                continue
            
            df = self._normalize_df(df)
            latest = latest_map.get(code)
            
            if latest is not None:
                mask = df['date'] > latest
                df = df[mask]
            
            if not df.empty:
                new_chunks.append(df)
                results[code] = len(df)
        
        # Step 3: ä¸€æ¬¡æ€§å†™å…¥
        if new_chunks:
            combined = pd.concat(new_chunks, ignore_index=True)
            self.bulk_upsert(combined)
            print(f"[DB] Incremental: {len(new_chunks)} stocks, {len(combined):,} new rows")
        
        return results

    # ==================== æŸ¥è¯¢æ¥å£ ====================
    def get_market_snapshot(self, target_date: Union[str, date]) -> pd.DataFrame:
        """
        ç§’çº§å…¨å¸‚åœºå¿«ç…§æŸ¥è¯¢
        
        æ€§èƒ½: 5000 stocks < 100ms (åˆ©ç”¨ idx_date ç´¢å¼•)
        """
        if isinstance(target_date, str):
            target_date = datetime.strptime(target_date, "%Y-%m-%d").date()
        
        with self.connect() as conn:
            return conn.execute(f"""
                SELECT code, market, date, open, high, low, close, vol, amount
                FROM {self.TABLE}
                WHERE date = ?
                ORDER BY code
            """, [target_date]).fetchdf()

    def get_stock_history(
        self,
        code: str,
        start: Optional[str] = None,
        end: Optional[str] = None
    ) -> pd.DataFrame:
        """å•è‚¡å†å²æ•°æ®æŸ¥è¯¢"""
        with self.connect() as conn:
            conditions = ["code = ?"]
            params: List = [code]
            
            if start:
                conditions.append("date >= ?")
                params.append(start)
            if end:
                conditions.append("date <= ?")
                params.append(end)
            
            where = " AND ".join(conditions)
            return conn.execute(f"""
                SELECT * FROM {self.TABLE}
                WHERE {where}
                ORDER BY date
            """, params).fetchdf()

    def get_multi_stock_panel(
        self,
        codes: List[str],
        start: str,
        end: str
    ) -> pd.DataFrame:
        """å¤šè‚¡é¢æ¿æ•°æ® (é€‚åˆå› å­è®¡ç®—)"""
        with self.connect() as conn:
            placeholders = ','.join(['?'] * len(codes))
            return conn.execute(f"""
                SELECT * FROM {self.TABLE}
                WHERE code IN ({placeholders})
                  AND date BETWEEN ? AND ?
                ORDER BY date, code
            """, codes + [start, end]).fetchdf()

    # ==================== ç»Ÿè®¡ä¸ç»´æŠ¤ ====================
    def get_stats(self) -> Dict:
        """æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        with self.connect() as conn:
            stats = conn.execute(f"""
                SELECT 
                    COUNT(*) as total_rows,
                    COUNT(DISTINCT code) as stocks,
                    MIN(date) as min_date,
                    MAX(date) as max_date,
                    COUNT(DISTINCT date) as trading_days
                FROM {self.TABLE}
            """).fetchone()
            
            return {
                "total_rows": stats[0],
                "unique_stocks": stats[1],
                "date_range": (stats[2], stats[3]),
                "trading_days": stats[4],
                "db_size_mb": round(os.path.getsize(self.db_path) / 1024**2, 2)
                              if os.path.exists(self.db_path) else 0
            }

    def vacuum(self) -> None:
        """å‹ç¼©æ•°æ®åº“æ–‡ä»¶"""
        with self.connect() as conn:
            conn.execute("VACUUM")

    # ==================== å†…éƒ¨æ–¹æ³• ====================
    @staticmethod
    def _normalize_df(df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ– DataFrame æ ¼å¼"""
        df = df.copy()
        
        # æ—¥æœŸè½¬æ¢
        if 'date' in df.columns:
            if df['date'].dtype == 'object':
                df['date'] = pd.to_datetime(df['date']).dt.date
            elif hasattr(df['date'].dtype, 'date'):
                df['date'] = df['date'].dt.date
        
        # åˆ—é¡ºåº
        cols = ['code', 'market', 'date', 'name', 'open', 'high', 'low', 'close', 'vol', 'amount']
        if 'name' not in df.columns:
            df['name'] = ""
        return df[[c for c in cols if c in df.columns]]


# ==================== å¿«æ·æ¥å£ ====================
def get_market_snapshot(
    target_date: Union[str, date],
    db_path: str = "stocks_daily.db"
) -> pd.DataFrame:
    """
    ä¸€è¡Œä»£ç è·å–å…¨å¸‚åœºå¿«ç…§
    
    Usage:
        df = get_market_snapshot('2024-01-15')
    """
    return StockDatabase(db_path).get_market_snapshot(target_date)


# ==================== å•å…ƒæµ‹è¯• ====================
import unittest

class TestStockDatabase(unittest.TestCase):
    """æ•°æ®åº“æ¨¡å—æµ‹è¯•"""
    
    TEST_DB = "test_stocks.db"
    
    @classmethod
    def setUpClass(cls) -> None:
        cls.db = StockDatabase(cls.TEST_DB)
        cls.mock_df = cls._generate_mock()
    
    @classmethod
    def tearDownClass(cls) -> None:
        if os.path.exists(cls.TEST_DB):
            os.remove(cls.TEST_DB)
    
    @staticmethod
    def _generate_mock(n_stocks: int = 100, n_days: int = 50) -> pd.DataFrame:
        import numpy as np
        rows = []
        dates = pd.bdate_range('2024-01-01', periods=n_days)
        for i in range(n_stocks):
            for d in dates:
                rows.append({
                    'code': f'{i:06d}', 'market': i % 2,
                    'date': d.date(), 'open': 10 + np.random.rand(),
                    'high': 11, 'low': 9, 'close': 10.5,
                    'vol': 1000000, 'amount': 10000000
                })
        return pd.DataFrame(rows)
    
    def test_01_bulk_upsert(self) -> None:
        """æµ‹è¯•æ‰¹é‡å†™å…¥"""
        rows = self.db.bulk_upsert(self.mock_df)
        self.assertEqual(rows, len(self.mock_df))
    
    def test_02_market_snapshot(self) -> None:
        """æµ‹è¯•å…¨å¸‚åœºå¿«ç…§"""
        df = self.db.get_market_snapshot('2024-01-02')
        self.assertGreater(len(df), 0)
        self.assertIn('code', df.columns)
    
    def test_03_snapshot_performance(self) -> None:
        """æµ‹è¯•å¿«ç…§æ€§èƒ½ < 1ç§’"""
        t0 = time.perf_counter()
        _ = self.db.get_market_snapshot('2024-01-15')
        elapsed = time.perf_counter() - t0
        self.assertLess(elapsed, 1.0)
    
    def test_04_incremental_update(self) -> None:
        """æµ‹è¯•å¢é‡æ›´æ–°"""
        # æ–°å¢æ•°æ®
        new_df = pd.DataFrame([{
            'code': '000001', 'market': 0, 'date': '2025-01-01',
            'open': 15, 'high': 16, 'low': 14, 'close': 15.5,
            'vol': 2000000, 'amount': 30000000
        }])
        result = self.db.incremental_update({'000001': new_df})
        self.assertEqual(result['000001'], 1)
    
    def test_05_upsert_dedup(self) -> None:
        """æµ‹è¯• UPSERT å»é‡"""
        initial = self.db.get_stats()['total_rows']
        self.db.bulk_upsert(self.mock_df)  # é‡å¤å†™å…¥
        final = self.db.get_stats()['total_rows']
        self.assertEqual(initial, final - 1)  # åªå¤šäº† test_04 çš„ 1 è¡Œ


# ==================== å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ ====================
if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    print("=" * 60)
    print("Running Unit Tests...")
    unittest.main(verbosity=2, exit=False, argv=[''])
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    print("\n" + "=" * 60)
    print("Performance Benchmark")
    print("=" * 60)
    
    import numpy as np
    
    # ç”Ÿæˆ 5000 è‚¡ * 2500 å¤© = 1250ä¸‡è¡Œ
    def gen_large_dataset() -> pd.DataFrame:
        n_stocks, n_days = 5000, 2500
        print(f"Generating {n_stocks:,} stocks Ã— {n_days:,} days = {n_stocks*n_days:,} rows...")
        
        dates = pd.bdate_range('2014-01-01', periods=n_days)
        data = {
            'code': np.repeat([f'{i:06d}' for i in range(n_stocks)], n_days),
            'market': np.tile(np.arange(n_stocks) % 2, n_days),
            'date': np.tile(dates, n_stocks),
            'open': np.random.uniform(5, 100, n_stocks * n_days).astype('float32'),
            'high': np.random.uniform(5, 100, n_stocks * n_days).astype('float32'),
            'low': np.random.uniform(5, 100, n_stocks * n_days).astype('float32'),
            'close': np.random.uniform(5, 100, n_stocks * n_days).astype('float32'),
            'vol': np.random.randint(10000, 100000000, n_stocks * n_days),
            'amount': np.random.randint(100000, 1000000000, n_stocks * n_days),
        }
        return pd.DataFrame(data)
    
    db = StockDatabase("benchmark.db")
    
    # 1. å†™å…¥æµ‹è¯•
    large_df = gen_large_dataset()
    t0 = time.perf_counter()
    rows = db.bulk_upsert(large_df)
    write_time = time.perf_counter() - t0
    print(f"\n[Write] {rows:,} rows in {write_time:.2f}s â†’ {rows/write_time:,.0f} rows/sec")
    
    # 2. å¿«ç…§æŸ¥è¯¢æµ‹è¯•
    t0 = time.perf_counter()
    snapshot = db.get_market_snapshot('2020-06-15')
    query_time = time.perf_counter() - t0
    print(f"[Query] Market snapshot ({len(snapshot):,} stocks) in {query_time*1000:.1f}ms")
    
    # 3. ç»Ÿè®¡ä¿¡æ¯
    print(f"\n[Stats] {db.get_stats()}")
    
    # æ¸…ç†
    os.remove("benchmark.db")